{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "certain-german",
   "metadata": {},
   "source": [
    "# Reducing Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-gauge",
   "metadata": {},
   "source": [
    "$\\partial L(y, y'| w)/\\partial w$ with $w=$ model parameters, e.g. weights + biases. This tells us how loss changes for a given example $y$ + its prediction $y'$. Repeatedly take small steps in the direction that minimises loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-emergency",
   "metadata": {},
   "source": [
    "### Iterative approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-lease",
   "metadata": {},
   "source": [
    " <img src=\"images/GradientDescentDiagram.svg\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-deployment",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "Regression problems yield convex loss vs weight plots.\n",
    "\n",
    "Convex $<=>$ 1 minimum\n",
    "\n",
    "Start from random weights $w$ values. Compute gradient of loss curve at that point. Gradient in direction $i$ is $\\nabla_iL=\\partial L/\\partial w_i$ and tells us which direction to go, $-\\nabla_iL$ points in the direction of greatest decrease of the function in the $i$th direction.\n",
    "\n",
    "The learning rate determines the step size that the weights are adjusted by in the direction $-\\nabla_iL$. If LR is too large then we overshoot the minimum, too small and convergence takes a long time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-spoke",
   "metadata": {},
   "source": [
    "### SGD & Mini-batch SGD\n",
    "In gradient descent, a batch is the total number of examples used to compute the graident in a single step. This should (mathematiclly speaking) be the entire dataset $D$. SGD is a stochastic approximation of graident descent, performed by replacing the actual gradient (over the entirety of $D$) with the gradient of a single example (picked at random). This can be noisy.\n",
    "\n",
    "Mini-batch SGD is a compromise between full-batch and SGD. Choosing a mini-batch of 10-1000 samples, chosen at random. It reduces the amount of noise (because e.g. a weight update made using a batch of 50 with 1 mislabelled sample is better than a weight update made using only the single mislabelled sample.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
